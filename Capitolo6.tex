\chapter{Appendici}





\section{SVD (Singular Value Decomposition)}
Consideriamo una matrice \(A \in \mathbb{R}^{m \times n}\).
Inoltre dati due vettori \(y \in \mathbb{R}^{m}\) 
e \(x \in \mathbb{R}^{n}\) tali che vale la relazione:
\[y = A x\]
\[A = U \Sigma V^{T}\]
\begin{itemize}
  \item La matrice \(U \in \mathbb{R}^{m \times m}\) (matrice di rotazione delle uscite) 
  è una matrice ortonormale
  composta dai vettori colonna \(u_{1}, u_{2}, \ldots, u_{m}\),
  si ha:
  \[U  =\begin{pmatrix}
    u_{1} & u_{2} & \ldots & u_{m}
  \end{pmatrix}\]
  \item La matrice \(V \in \mathbb{R}^{n \times n}\) 
  (matrice di rotazione degli ingressi) 
  è una matrice ortonormale
  composta dai vettori colonna \(v_{1}, v_{2}, \ldots, v_{n}\),
  si ha:
  \[V  =\begin{pmatrix}
    v_{1} & v_{2} & \ldots & v_{n}
  \end{pmatrix}\]
  \item La matrice \(\Sigma 
  \in \mathbb{R}^{m \times n}\) (matrice dei valori singolari) 

  \[p= min\left\{m,n\right\}\]
   Dove si ha che:
  \[\Sigma =  \begin{pmatrix}
    \Sigma_{1} & 0
  \end{pmatrix} , \hspace{10pt}
    \textnormal{ se } p=m
  \]
  \[\Sigma = \begin{pmatrix}
    \Sigma_{1} \\ 0
  \end{pmatrix} , \hspace{10pt}
    \textnormal{ se } p=n
  \]
\end{itemize}


\[y = \begin{pmatrix}
  u_{1} & u_{2} & \ldots & u_{m} 
\end{pmatrix}
  \begin{pmatrix}
    \Sigma_{1} & 0 & \ldots & 0 \\
    0 & \Sigma_{2} & \ldots & 0 \\
    \vdots & \vdots & \ddots & \vdots \\
    0 & 0 & \ldots & \Sigma_{m} \\
  \end{pmatrix}
\begin{pmatrix}
  v_{1}^{T} \\ 
  v_{2}^{T} \\
  \vdots \\
  v_{n}^{T}
\end{pmatrix}
 \cdot x
\]

Se io prendo \(x=v_{i}\) si ha:
\[
  \begin{pmatrix}
  v_{1}^{T} \\ 
  v_{2}^{T} \\
  \vdots \\
  v_{n}^{T}
\end{pmatrix}
\cdot 
x =
  \begin{pmatrix}
  v_{1}^{T} \\ 
  v_{2}^{T} \\
  \vdots \\
  v_{n}^{T}
\end{pmatrix}
\cdot 
v_{i} 
\]
Ricordando che \(V\) è ortonormale le sue colonne 
sono tra loro ortogonali e di norma 1, dunque si ha:
\[ v^{T}_{j} \cdot v_{i} = 0 , \hspace{10pt} \textnormal{se } i \neq j \]
\[ v^{T}_{j} \cdot v_{i} = 1 , \hspace{10pt} \textnormal{se } i = j \]
Dunque si ha:
\[
  \begin{pmatrix}
  v_{1}^{T} \\ 
  v_{2}^{T} \\
  \vdots \\
  v_{i}^{T} \\
  \vdots \\
  v_{n}^{T}
\end{pmatrix}
\cdot 
v_{i} 
=
  \begin{pmatrix}
  0 \\ 
  0 \\
  \vdots \\
  1 \\
  \vdots \\
  0
\end{pmatrix}
\]
Dove l'uno è nella \(i\)-esima riga.
Andiamo a calcolarci la norma 2 di \(A\):
\[\left|\left|A \right|\right|_{2} = \sqrt{\lambda_{MAX}
\left(A^{T} A\right) } \]
Notiamo che per le proprietà della trasposta \(A^{T}= 
\left( U \Sigma V\right)^{T}  =
 V \Sigma^{T} U^{T} \)
\[ \left|\left|A \right|\right|_{2} =
\sqrt{\lambda_{MAX} \left(V 
\Sigma^{T} U^{T} U \Sigma V^{T}\right)} \]
Ora ricordando che \(U\) è ortonormale si ha \(U^{T} U = I\) 
(\(U^{T} = U^{-1}\)):
\[\left|\left|A \right|\right|_{2} 
=
\sqrt{\lambda_{MAX} \left(V \Sigma^{T} \Sigma  V^{T}\right)}\]
Ora ricordando che \(\Sigma\) 
e \(\Sigma^{T}\) sono diagonali si ha 
che vale la proprietà commutativa tra le matrici, dunque possiamo scrivere:
\[\left|\left|A \right|\right|_{2} 
=
\sqrt{\lambda_{MAX} \left(\Sigma^{T} \Sigma V V^{T}\right)}\]
Ora ricordando che \(\Sigma\) è diagonale vale
\(\Sigma^{T} = \Sigma\), dunque si ha:
\[\left|\left|A \right|\right|_{2} = 
\sqrt{\lambda_{MAX} \Sigma^{T} \Sigma }
= \sqrt{\lambda_{MAX} \Sigma \Sigma }
=  \sqrt{\lambda_{MAX} \Sigma^{2}}
\]
Come si vede la norma 2 di \(A\)
è legata al valore singolare maggiore di \(A\).

\begin{figure}[htbp] % [h] suggerisce a LaTeX di mettere la figura "qui" (here)
    \centering
\begin{tikzpicture}[x=2cm, y=2cm]
        
        % --- ASSI (Mantengo le frecce standard qui) ---
        \draw[thick, ->] (-1.8,0) -- (1.8,0) node[right] {$x_1$};
        \draw[thick, ->] (0,-1.8) -- (0,1.8) node[above] {$x_2$};

        % Cerchio unitario (raggio matematico = 1)
        \draw[thick, dashed, black] (0,0) circle (1);
        \node[black, below right] at (-0.7, -0.7) {$r=1$};

        % --- VETTORI CON NUOVE PUNTE ---
        % Nota: Ho cambiato '->' in '-stealth' per le punte dei vettori

        % Vettore x blu, NORMA MATEMATICA 1, Primo quadrante (60 gradi)
        \draw[-stealth, very thick, blue] (0,0) -- (60:1) node[above right] {$\mathbf{x}$};

        % --- MODIFICA QUI ---
        % Vettore y rosso, NORMA MATEMATICA 1.25 (>1)
        % Spostato nel SECONDO QUADRANTE (es. 135 gradi)
        % x1 negativo, x2 positivo
        \draw[-stealth, very thick, red] (0,0) -- (135:2) node[above left] {$\mathbf{y}$};

    \end{tikzpicture}
    \caption{Esempio di rotazione 
    del vettore di ingresso \(x = v_{1}\)
     nel vettore di uscita 
    amplificato \(y = \sigma_{1} u_{1}\). In questo caso abbiamo preso \(m=n=2\),
    dunque la matrice \(A\) è quadrata, mentre \(\Sigma\) è diagonale. Notiamo
    che abbiamo potuto rappresentare tutto sullo stesso grafico solo 
    perché \(m=n\).}
    \label{fig:cerchio_assi}
\end{figure}




\section{Diagonalizzare}

Sia dato un sistema LTI descritto dalle equazioni di stato:
\[
\dot{x}(t) = Ax(t) + Bu(t)
\]
quando noi diamo un sistema in questa forma diamo per assodato che il vettore 
\(x\) sia scritto nella base canonica di \(\mathbb{R}^{n}\).
Se la matrice \(A\) è diagonalizzabile, ci poniamo il problema di portare \(x\)
in un nuovo stato \(\hat{x}\) in cui \(A\) è diagonale. 
Dunque cerchiamo una matrice di trasformazione \(T\) tale che:
\[ x = T \hat{x} \Longrightarrow
\hat{A} = T^{-1} A T \textnormal{ è diagonale}\] 
Ma come si può immediatamente notare la matrice \(T\) è la matrice del cambiamento 
di base dalla base canonica alla base che diagonalizza \(A\).
Quindi a noi basta trovare la matrice che permette di portare il vettore \(\hat{x}\),
scritto nella base che diagonalizza \(A\), 
nel vettore \(x\) scritto nella base canonica.
Ma la base che diagonalizza \(A\) è formata dagli autovettori di \(A\),
dunque scegliamo un insieme di autovettori linearmente indipendenti di \(A\):
\[ \hat{B} = \left\{v_{1}, v_{2}, \ldots, v_{n}\right\} \] 
Per calcolare \(T\) dobbiamo ricordare che la matrice del cambiamento di base
si calcola attraverso l'applicazione identità (che è un applicazione lineare),
dunque applichiamo l'identità agli autovettori scelti:
\[ id(\hat{B}) =\left\{v_{1} , v_{2} , \ldots , v_{n}\right\} \]
Ora scomponiamo i vettori rispetto alla base canonica 
ed incolonniamoli come colonne della matrice \(T\)(
la base canonica è molto comoda perchè ci permette di scrivere i vettori
proprio come sono), dunque la matrice \(T\) vale:
\[ T = 
\begin{pmatrix}
    v_{1} & v_{2} & \ldots & v_{n}
\end{pmatrix}\]
In cui i vettori \(v_{1}, v_{2}, \ldots , v_{n} \in \mathbb{R}^{n \times 1}\)
sono vettori colonna \(n \times 1\), dunque la matrice \(T \in \mathbb{R}^{n \times n}\).











\section{Uguaglianza matriciale per dimostrare la relazione di \(T(s)\)}
Per dimostrare che per la
funzione di sensitività complementare \(T(s)\) vale la relazione:
\[T(s) = \left(I + L(s)\right)^{-1}L(s) = L(s) \left(I + L(s)\right)^{-1}\]
cioè che le matrici \(\left(I + L(s)\right)^{-1}\) e \(L(s)\) commutano,
abbiamo usato la seguente identità matriciale:
\[\left(I +AB\right)^{-1} A = A \left(I+BA\right)^{-1}\]
\begin{proof}
    Noi vogliamo dimostrare che:
    \[\left(I +AB\right)^{-1} A = A \left(I+BA\right)^{-1}\]
    Studiamoci la matricie differenza:
    \[D \coloneq \left(I +AB\right)^{-1} A - A \left(I+BA\right)^{-1}\]
    Se riusciamo a dimostrare che \(D=0\) allora abbiamo dimostrato l'uguaglianza.
    Moltiplichiamo \(D\) a sinistra per \(\left(I +AB\right)\) e a destra per 
    \(\left(I+BA\right)\), si ha:
    \[(I+AB) D (I + BA)=(I+AB)\left(I +AB\right)^{-1} A (I+BA) -(I + AB)
     A \left(I+BA\right)^{-1}(I+BA)\]
    Notiamo che \(\left(I +AB\right)^{-1} A (I+BA) = I\) e \(\left(I+BA\right)^{-1}(I+BA) = I\),
    quindi si ha:
    \[(I+AB) D (I + BA)= A (I+BA) - (I + AB) A = A + ABA - A - ABA = 0\]
    Abbiamo dimostrato \(D=0\), quindi anche l'uguaglianza iniziale.
\end{proof}