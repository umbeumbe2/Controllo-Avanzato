\chapter{Appendici}



\section{Norme}
Andiamo ora a definire le norme utilizzate in questi appunti.
Per prima andiamo a definire le norme su vettori di 
\(\mathbb{R}^{n}\). Dato un vettore \(x \in \mathbb{R}^{n}\),
la norma \(p\) di \(x\) (i vettori li consideriamo 
vettori colonna \(x \in \mathbb{R}^{n \times 1}\)) è definita come:
\[\left|\left| x \right|\right|_{p} =
\left( \sum_{i=1}^{n} |x_{i}|^{p} \right)^{\frac{1}{p}} \]
Norme molto usate sono la norma 1:
\[\left|\left| x \right|\right|_{1} =
\sum_{i=1}^{n} |x_{i}| \]
e la norma 2:
\[\left|\left| x \right|\right|_{2} =
\sqrt{ \sum_{i=1}^{n} |x_{i}|^{2} } = \sqrt{x^{T} \cdot x}\]
Come possiamo vedere maggiore è il valore di \(p\) 
maggiore è il peso dato agli elementi di \(x\) con 
valore assoluto maggiore. Portando \(p\) a infinito 
si definisce la norma infinito come:
\[\left|\left| x \right|\right|_{\infty} =
\max_{i} |x_{i}| \]

Andiamo ora a definire la norma di un operatore, 
detta anche norma indotta o norma operazionale, a partire 
dalla definizione di norma di un vettore. 
Consideriamo 
un operatore (operatore è sinonimo di funzione) 
\(S: \mathbb{R}^{m} \to \mathbb{R}^{n}\).
Detta \(y\) l'immagine del vettore \(u\) attraverso \(S\),
cioè:
\[y = S(u)\]
La \(p\)-norma indotta di \(S\) è definita come:
\[\left|\left| S \right|\right|_{p} =
\sup_{u \neq 0}
\frac{\left|\left| y \right|\right|_{p}}
{\left|\left| u \right|\right|_{p}}
\]
Se \(S\) è un operatore lineare 
allora è rappresentato dalla matrice \(A \in \mathbb{R}^{n \times m}\),
e si ha:
\[y = A u\]
La \(p\)-norma indotta di \(A\) si scrive come:
\[\left|\left| A \right|\right|_{p} =
\sup_{u \neq 0}
\frac{\left|\left| A u \right|\right|_{p}}
{\left|\left| u \right|\right|_{p}}
\]
Ora riscriviamo la \(p\)-norma indotta di \(A\) in un altro modo.
Partiamo dal dire che \(\forall u \in \mathbb{R}^{n}\) esistono
almeno un \(\rho \in \mathbb{R}\) ed un \(\bar{u} \in \mathbb{R}^{n}\) 
tali che:
\begin{equation}
  u = \rho \bar{u}
  \label{u_riscritta}
\end{equation}
con \(\left|\left| \bar{u} \right|\right|_{p} = 1\) 
( infatti basta prendere \(\rho = \left|\left| u \right|\right|_{p}\)
e \(\bar{u} = \frac{u}{\left|\left| u \right|\right|_{p}}\)).
Andiamo ora a vedere
che la funzione di cui stiamo andando a cercare l'estremo 
superiore non varia al variare di \(\rho\):
\[
\frac{\left|\left| A \rho \bar{u} \right|\right|_{p}}
{\left|\left| \rho \bar{u} \right|\right|_{p}} 
\]
Il fattore \(\rho\) è uno scalare e dunque può essere
portato fuori dalla norma, e dopo si può semplificare,
dunque si ha:
\[
\frac{\left|\left| A \rho \bar{u} \right|\right|_{p}}
{\left|\left| \rho \bar{u} \right|\right|_{p}}  
= 
\frac{\left|\left| A \bar{u} \right|\right|_{p}}
{\left|\left| \bar{u} \right|\right|_{p}} 
\]
inoltre vale \(\left|\left| \bar{u} \right|\right|_{p} = 1\), dunque si ha:
\[
\frac{\left|\left| A \rho \bar{u} \right|\right|_{p}}
{\left|\left| \rho \bar{u} \right|\right|_{p}} 
=
\left|\left| A \bar{u} \right|\right|_{p}
\]
Dunque come possiamo vedere due vettori che hanno lo stesso 
\(\bar{u}\) ma con \(\rho\) diverso 
(dunque due vettori 
con stessa direzione ma lunghezza diversa) 
danno lo stesso valore 
alla funzione di cui stiamo cercando l'estremo superiore.
Dunque invece di cercare l'estremo superiore su tutti i vettori 
\(u\) con norma diversa da zero, ci basta prendere 
un solo rappresentante per ogni \(\bar{u}\).
Prendiamo come rappresentante proprio il vettore \(u\)
che ha \(\rho = 1\) (dunque vale \(u = \bar{u}\)),
il sup si può riscrivere come:


\[\left|\left| A \right|\right|_{p} =
\sup_{u \neq 0}
\frac{\left|\left| A u \right|\right|_{p}}
{\left|\left| u \right|\right|_{p}}
= \sup_{\left|\left|\bar{u}\right|\right|_{p} = 1} 
\left|\left| A \bar{u} \right|\right|_{p}
\]

Andiamo a caratterizzarci alcuni casi particolari di norme indotte:

\begin{itemize}
  \item Norma 1 indotta: \(\left|\left| A \right|\right|_{1}\):
\[
\left|\left| A \right|\right|_{1} =
\sup_{\left|\left|\bar{u}\right|\right|_{1} = 1} 
\left|\left| A \bar{u} \right|\right|_{1} 
\]
Se indichiamo con \(a_{ij}\) l'elemento nella riga \(i\)-esima
e colonna \(j\)-esima della matrice \(A\),
allora si ha che la norma 1 indotta di \(A\) vale:
\[
\left|\left| A \right|\right|_{1} =
\sup_{\left|\left|\bar{u}\right|\right|_{1} = 1} 
\left|  \sum_{j=1}^{m} A(:,j) \bar{u}_{j} \right|_{1}
\]
dove con \(A(:, j)\) indichiamo la \(j\)-esima colonna di \(A\).
Per la disuguaglianza triangolare si ha:
\[ 
\left|  \sum_{j=1}^{m} A(:,j) \bar{u}_{j} \right|_{1}
\le \sum_{j=1}^{m} \left| A(:,j) \bar{u}_{j} \right|_{1} 
= \sum_{j=1}^{m} \left| \bar{u}_{j} \right| \left| A(:,j) \right|_{1},
\qquad \forall \bar{u} : \left|\left|\bar{u}\right|\right|_{1} = 1
\]
Praticamente stiamo dicendo che 
per ogni vettore \(\bar{u}\) la sommatoria è 
minore uguale della combinazione lineare, con 
coefficienti \(\left| \bar{u}_{j} \right|\),
 delle norme 1 delle colonne di \(A\). Possiamo quindi 
 scrivere che:
\[ 
\sum_{j=1}^{m} \left| \bar{u}_{j} \right| \left| A(:,j) \right|_{1}
\le \max_{j=1, \dots, m} \left| A(:,j) \right|_{1} \]
Dunque si ha che il \(\sup\) è proprio:
\[
\left|\left| A \right|\right|_{1} =
\sup_{\left|\left|\bar{u}\right|\right|_{1} = 1} 
\left|\left| A \bar{u} \right|\right|_{1} 
= \max_{j=1, \dots, m} \left| A(:,j) \right|_{1}
= \max_{j=1, \dots, m} \sum_{i=1}^{n} |a_{ij}|
\]

\item Norma 2 indotta: \(\left|\left| A \right|\right|_{2}\):


\item Norma infinito indotta: \(\left|\left| A \right|\right|_{\infty}\):

\end{itemize}

\section{SVD (Singular Value Decomposition)}
Consideriamo una matrice \(A \in \mathbb{R}^{m \times n}\).
Inoltre dati due vettori \(y \in \mathbb{R}^{m}\) 
e \(x \in \mathbb{R}^{n}\) tali che vale la relazione:
\[y = A x\]
\[A = U \Sigma V^{T}\]
\begin{itemize}
  \item La matrice \(U \in \mathbb{R}^{m \times m}\) (matrice di rotazione delle uscite) 
  è una matrice ortonormale
  composta dai vettori colonna \(u_{1}, u_{2}, \ldots, u_{m}\),
  si ha:
  \[U  =\begin{pmatrix}
    u_{1} & u_{2} & \ldots & u_{m}
  \end{pmatrix}\]
  \item La matrice \(V \in \mathbb{R}^{n \times n}\) 
  (matrice di rotazione degli ingressi) 
  è una matrice ortonormale
  composta dai vettori colonna \(v_{1}, v_{2}, \ldots, v_{n}\),
  si ha:
  \[V  =\begin{pmatrix}
    v_{1} & v_{2} & \ldots & v_{n}
  \end{pmatrix}\]
  \item La matrice \(\Sigma 
  \in \mathbb{R}^{m \times n}\) (matrice dei valori singolari) 

  \[p= min\left\{m,n\right\}\]
   Dove si ha che:
  \[\Sigma =  \begin{pmatrix}
    \Sigma_{1} & 0
  \end{pmatrix} , \hspace{10pt}
    \textnormal{ se } p=m
  \]
  \[\Sigma = \begin{pmatrix}
    \Sigma_{1} \\ 0
  \end{pmatrix} , \hspace{10pt}
    \textnormal{ se } p=n
  \]
\end{itemize}


\[y = \begin{pmatrix}
  u_{1} & u_{2} & \ldots & u_{m} 
\end{pmatrix}
  \begin{pmatrix}
    \Sigma_{1} & 0 & \ldots & 0 \\
    0 & \Sigma_{2} & \ldots & 0 \\
    \vdots & \vdots & \ddots & \vdots \\
    0 & 0 & \ldots & \Sigma_{m} \\
  \end{pmatrix}
\begin{pmatrix}
  v_{1}^{T} \\ 
  v_{2}^{T} \\
  \vdots \\
  v_{n}^{T}
\end{pmatrix}
 \cdot x
\]

Se io prendo \(x=v_{i}\) si ha:
\[
  \begin{pmatrix}
  v_{1}^{T} \\ 
  v_{2}^{T} \\
  \vdots \\
  v_{n}^{T}
\end{pmatrix}
\cdot 
x =
  \begin{pmatrix}
  v_{1}^{T} \\ 
  v_{2}^{T} \\
  \vdots \\
  v_{n}^{T}
\end{pmatrix}
\cdot 
v_{i} 
\]
Ricordando che \(V\) è ortonormale le sue colonne 
sono tra loro ortogonali e di norma 1, dunque si ha:
\[ v^{T}_{j} \cdot v_{i} = 0 , \hspace{10pt} \textnormal{se } i \neq j \]
\[ v^{T}_{j} \cdot v_{i} = 1 , \hspace{10pt} \textnormal{se } i = j \]
Dunque si ha:
\[
  \begin{pmatrix}
  v_{1}^{T} \\ 
  v_{2}^{T} \\
  \vdots \\
  v_{i}^{T} \\
  \vdots \\
  v_{n}^{T}
\end{pmatrix}
\cdot 
v_{i} 
=
  \begin{pmatrix}
  0 \\ 
  0 \\
  \vdots \\
  1 \\
  \vdots \\
  0
\end{pmatrix}
\]
Dove l'uno è nella \(i\)-esima riga.
Andiamo a calcolarci la norma 2 di \(A\):
\[\left|\left|A \right|\right|_{2} = \sqrt{\lambda_{MAX}
\left(A^{T} A\right) } \]
Notiamo che per le proprietà della trasposta \(A^{T}= 
\left( U \Sigma V\right)^{T}  =
 V \Sigma^{T} U^{T} \)
\[ \left|\left|A \right|\right|_{2} =
\sqrt{\lambda_{MAX} \left(V 
\Sigma^{T} U^{T} U \Sigma V^{T}\right)} \]
Ora ricordando che \(U\) è ortonormale si ha \(U^{T} U = I\) 
(\(U^{T} = U^{-1}\)):
\[\left|\left|A \right|\right|_{2} 
=
\sqrt{\lambda_{MAX} \left(V \Sigma^{T} \Sigma  V^{T}\right)}\]
Ora ricordando che \(\Sigma\) 
e \(\Sigma^{T}\) sono diagonali si ha 
che vale la proprietà commutativa tra le matrici, dunque possiamo scrivere:
\[\left|\left|A \right|\right|_{2} 
=
\sqrt{\lambda_{MAX} \left(\Sigma^{T} \Sigma V V^{T}\right)}\]
Ora ricordando che \(\Sigma\) è diagonale vale
\(\Sigma^{T} = \Sigma\), dunque si ha:
\[\left|\left|A \right|\right|_{2} = 
\sqrt{\lambda_{MAX} \Sigma^{T} \Sigma }
= \sqrt{\lambda_{MAX} \Sigma \Sigma }
=  \sqrt{\lambda_{MAX} \Sigma^{2}}
\]
Come si vede la norma 2 di \(A\)
è legata al valore singolare maggiore di \(A\).

\begin{figure}[htbp] % [h] suggerisce a LaTeX di mettere la figura "qui" (here)
    \centering
\begin{tikzpicture}[x=2cm, y=2cm]
        
        % --- ASSI (Mantengo le frecce standard qui) ---
        \draw[thick, ->] (-1.8,0) -- (1.8,0) node[right] {$x_1$};
        \draw[thick, ->] (0,-1.8) -- (0,1.8) node[above] {$x_2$};

        % Cerchio unitario (raggio matematico = 1)
        \draw[thick, dashed, black] (0,0) circle (1);
        \node[black, below right] at (-0.7, -0.7) {$r=1$};

        % --- VETTORI CON NUOVE PUNTE ---
        % Nota: Ho cambiato '->' in '-stealth' per le punte dei vettori

        % Vettore x blu, NORMA MATEMATICA 1, Primo quadrante (60 gradi)
        \draw[-stealth, very thick, blue] (0,0) -- (60:1) node[above right] {$\mathbf{x}$};

        % --- MODIFICA QUI ---
        % Vettore y rosso, NORMA MATEMATICA 1.25 (>1)
        % Spostato nel SECONDO QUADRANTE (es. 135 gradi)
        % x1 negativo, x2 positivo
        \draw[-stealth, very thick, red] (0,0) -- (135:2) node[above left] {$\mathbf{y}$};

    \end{tikzpicture}
    \caption{Esempio di rotazione 
    del vettore di ingresso \(x = v_{1}\)
     nel vettore di uscita 
    amplificato \(y = \sigma_{1} u_{1}\). In questo caso abbiamo preso \(m=n=2\),
    dunque la matrice \(A\) è quadrata, mentre \(\Sigma\) è diagonale. Notiamo
    che abbiamo potuto rappresentare tutto sullo stesso grafico solo 
    perché \(m=n\).}
    \label{fig:cerchio_assi}
\end{figure}




\section{Diagonalizzare}

Sia dato un sistema LTI descritto dalle equazioni di stato:
\[
\dot{x}(t) = Ax(t) + Bu(t)
\]
quando noi diamo un sistema in questa forma diamo per assodato che il vettore 
\(x\) sia scritto nella base canonica di \(\mathbb{R}^{n}\).
Se la matrice \(A\) è diagonalizzabile, ci poniamo il problema di portare \(x\)
in un nuovo stato \(\hat{x}\) in cui \(A\) è diagonale. 
Dunque cerchiamo una matrice di trasformazione \(T\) tale che:
\[ x = T \hat{x} \Longrightarrow
\hat{A} = T^{-1} A T \textnormal{ è diagonale}\] 
Ma come si può immediatamente notare la matrice \(T\) è la matrice del cambiamento 
di base dalla base canonica alla base che diagonalizza \(A\).
Quindi a noi basta trovare la matrice che permette di portare il vettore \(\hat{x}\),
scritto nella base che diagonalizza \(A\), 
nel vettore \(x\) scritto nella base canonica.
Ma la base che diagonalizza \(A\) è formata dagli autovettori di \(A\),
dunque scegliamo un insieme di autovettori linearmente indipendenti di \(A\):
\[ \hat{B} = \left\{v_{1}, v_{2}, \ldots, v_{n}\right\} \] 
Per calcolare \(T\) dobbiamo ricordare che la matrice del cambiamento di base
si calcola attraverso l'applicazione identità (che è un applicazione lineare),
dunque applichiamo l'identità agli autovettori scelti:
\[ id(\hat{B}) =\left\{v_{1} , v_{2} , \ldots , v_{n}\right\} \]
Ora scomponiamo i vettori rispetto alla base canonica 
ed incolonniamoli come colonne della matrice \(T\)(
la base canonica è molto comoda perchè ci permette di scrivere i vettori
proprio come sono), dunque la matrice \(T\) vale:
\[ T = 
\begin{pmatrix}
    v_{1} & v_{2} & \ldots & v_{n}
\end{pmatrix}\]
In cui i vettori \(v_{1}, v_{2}, \ldots , v_{n} \in \mathbb{R}^{n \times 1}\)
sono vettori colonna \(n \times 1\), dunque la matrice \(T \in \mathbb{R}^{n \times n}\).











\section{Uguaglianza matriciale per dimostrare la relazione di \(T(s)\)}
Per dimostrare che per la
funzione di sensitività complementare \(T(s)\) vale la relazione:
\[T(s) = \left(I + L(s)\right)^{-1}L(s) = L(s) \left(I + L(s)\right)^{-1}\]
cioè che le matrici \(\left(I + L(s)\right)^{-1}\) e \(L(s)\) commutano,
abbiamo usato la seguente identità matriciale:
\[\left(I +AB\right)^{-1} A = A \left(I+BA\right)^{-1}\]
\begin{proof}
    Noi vogliamo dimostrare che:
    \[\left(I +AB\right)^{-1} A = A \left(I+BA\right)^{-1}\]
    Studiamoci la matricie differenza:
    \[D \coloneq \left(I +AB\right)^{-1} A - A \left(I+BA\right)^{-1}\]
    Se riusciamo a dimostrare che \(D=0\) allora abbiamo dimostrato l'uguaglianza.
    Moltiplichiamo \(D\) a sinistra per \(\left(I +AB\right)\) e a destra per 
    \(\left(I+BA\right)\), si ha:
    \[(I+AB) D (I + BA)=(I+AB)\left(I +AB\right)^{-1} A (I+BA) -(I + AB)
     A \left(I+BA\right)^{-1}(I+BA)\]
    Notiamo che \(\left(I +AB\right)^{-1} A (I+BA) = I\) e \(\left(I+BA\right)^{-1}(I+BA) = I\),
    quindi si ha:
    \[(I+AB) D (I + BA)= A (I+BA) - (I + AB) A = A + ABA - A - ABA = 0\]
    Abbiamo dimostrato \(D=0\), quindi anche l'uguaglianza iniziale.
\end{proof}


